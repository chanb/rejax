{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b257af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.95\"\n",
    "\n",
    "from functools import partial\n",
    "from rejax import get_algo\n",
    "from rejax.evaluate import evaluate, EvalState\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import _pickle as pickle\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ef8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"lqr\"\n",
    "data_dir = f\"/home/bryanpu1/projects/aaai_2026/data/{env_name}\"\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6446cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./configs/custom/{}.yaml\".format(env_name)\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f.read())\n",
    "\n",
    "algo_name = \"ppo\"\n",
    "config = config[algo_name]\n",
    "num_seeds = 1000\n",
    "\n",
    "num_files = 5\n",
    "starting_seed_id = 0\n",
    "\n",
    "max_steps_in_episode = 200\n",
    "\n",
    "if env_name == \"cartpole\":\n",
    "    from gymnax.environments.classic_control.cartpole import EnvParams, CartPole\n",
    "    env = CartPole()\n",
    "\n",
    "    def sample_env_params(key):\n",
    "        return {\n",
    "            \"gravity\": jax.random.uniform(key, shape=(num_seeds,)) * 10.0\n",
    "        }\n",
    "\n",
    "    def apply_env_params(algo_cls, env_params):\n",
    "        return algo_cls.create(\n",
    "            **config,\n",
    "            env_params=EnvParams(\n",
    "                gravity=env_params[\"gravity\"],\n",
    "                max_steps_in_episode=max_steps_in_episode,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "elif env_name.startswith(\"lqr\"):\n",
    "    env_setting = dict(\n",
    "        dim_x=3,\n",
    "        dim_u=3,\n",
    "        x_thres=1e-2,\n",
    "        sigma_w=0.0,\n",
    "        std_x=1.0,\n",
    "    )\n",
    "\n",
    "    kv_pairs = env_name.split(\"-\")[1:]\n",
    "    for kv_pair in kv_pairs:\n",
    "        (key, val) = kv_pair.split(\"=\")\n",
    "\n",
    "        if key in [\"dim_u\", \"dim_x\"]:\n",
    "            env_setting[key] = int(val)\n",
    "        elif key in [\"x_thres\", \"sigma_w\", \"std_x\"]:\n",
    "            env_setting[key] = float(val)\n",
    "\n",
    "    from rejax.envs.lqr import (\n",
    "        EnvParams,\n",
    "        DiscreteTimeLQR,\n",
    "        is_controllable,\n",
    "        is_stable,\n",
    "    )\n",
    "\n",
    "    dim_x, dim_u = env_setting[\"dim_x\"], env_setting[\"dim_u\"]\n",
    "    x_thres = env_setting[\"x_thres\"]\n",
    "    sigma_w = env_setting[\"sigma_w\"]\n",
    "    std_x = env_setting[\"std_x\"]\n",
    "    env = DiscreteTimeLQR(dim_x, dim_u)\n",
    "\n",
    "    def sample_env_params(key):\n",
    "        env_params = {\n",
    "            \"A\": [],\n",
    "            \"B\": [],\n",
    "        }\n",
    "        for seed_i in range(num_seeds):\n",
    "            key = jax.random.fold_in(key, seed_i)\n",
    "            env_params[\"A\"].append(jnp.zeros((dim_x, dim_x)))\n",
    "            env_params[\"B\"].append(jnp.zeros((dim_u, dim_u)))\n",
    "            \n",
    "            env_key = key\n",
    "            while not is_controllable(\n",
    "                env_params[\"A\"][-1],\n",
    "                env_params[\"B\"][-1],\n",
    "            ) or not is_stable(\n",
    "                env_params[\"A\"][-1],\n",
    "                env_params[\"B\"][-1],\n",
    "            ):\n",
    "                env_key, _ = jax.random.split(env_key)\n",
    "                env_params[\"A\"][-1] = jnp.tanh(jax.random.normal(\n",
    "                    jax.random.fold_in(env_key, 0),\n",
    "                    shape=(dim_x, dim_x),\n",
    "                ))\n",
    "                env_params[\"B\"][-1] = jnp.tanh(jax.random.normal(\n",
    "                    jax.random.fold_in(env_key, 1),\n",
    "                    shape=(dim_u, dim_u),\n",
    "                ))\n",
    "        return {\n",
    "            **{k: jnp.stack(v) for k, v in env_params.items()},\n",
    "            \"Q\": jnp.concatenate([jnp.diag(\n",
    "                1.0 - jax.random.uniform(\n",
    "                    jax.random.fold_in(jax.random.fold_in(key, 2), seed),\n",
    "                    shape=(dim_x),\n",
    "                )\n",
    "            )[None] for seed in range(num_seeds)], axis=0),\n",
    "            \"R\": jnp.concatenate([jnp.diag(\n",
    "                1.0 - jax.random.uniform(\n",
    "                    jax.random.fold_in(jax.random.fold_in(key, 3), seed),\n",
    "                    shape=(dim_u),\n",
    "                )\n",
    "            )[None] for seed in range(num_seeds)], axis=0),\n",
    "        }\n",
    "\n",
    "    def apply_env_params(algo_cls, env_params):\n",
    "        return algo_cls.create(\n",
    "            **config,\n",
    "            env_params=EnvParams(\n",
    "                x_thres=x_thres,\n",
    "                max_steps_in_episode=max_steps_in_episode,\n",
    "                sigma_w=sigma_w,\n",
    "                std_x=std_x,\n",
    "                A=env_params[\"A\"],\n",
    "                B=env_params[\"B\"],\n",
    "                Q=env_params[\"Q\"],\n",
    "                R=env_params[\"R\"],\n",
    "            ),\n",
    "        )\n",
    "    \n",
    "    from scipy.linalg import solve_discrete_are\n",
    "    def get_opt_controllers(env_params):\n",
    "        opt_controllers = []\n",
    "        for (A, B, Q, R) in zip(\n",
    "            env_params[\"A\"],\n",
    "            env_params[\"B\"],\n",
    "            env_params[\"Q\"],\n",
    "            env_params[\"R\"],\n",
    "        ):\n",
    "            opt_cost = solve_discrete_are(A, B, Q, R)\n",
    "\n",
    "            opt_controller = -np.linalg.pinv(\n",
    "                R + B.T @ opt_cost @ B @ B.T @ opt_cost @ A\n",
    "            )\n",
    "            opt_controllers.append(opt_controller[None])\n",
    "        return np.concatenate(\n",
    "            opt_controllers, axis=0\n",
    "        )\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "config[\"env\"] = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cace57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_kwargs': {'activation': 'tanh'},\n",
       " 'num_envs': 1,\n",
       " 'num_steps': 100,\n",
       " 'num_epochs': 5,\n",
       " 'num_minibatches': 5,\n",
       " 'learning_rate': 0.0003,\n",
       " 'max_grad_norm': 0.5,\n",
       " 'total_timesteps': 25000,\n",
       " 'eval_freq': 5000,\n",
       " 'gamma': 0.95,\n",
       " 'gae_lambda': 0.95,\n",
       " 'clip_eps': 0.2,\n",
       " 'ent_coef': 0.01,\n",
       " 'vf_coef': 0.5,\n",
       " 'buffer_size': 25000,\n",
       " 'env': <rejax.envs.lqr.DiscreteTimeLQR at 0x7ac29c3b7910>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ebb324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19857ed2b87d47ea81db99afe1dfbb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file_i in tqdm(range(num_files)):\n",
    "    seed_id = starting_seed_id + file_i\n",
    "\n",
    "    key = jax.random.PRNGKey(seed_id)\n",
    "    keys = jax.random.split(key, num_seeds)\n",
    "\n",
    "    env_params = sample_env_params(key)\n",
    "\n",
    "    algo_cls = get_algo(algo_name)\n",
    "    algo = jax.vmap(\n",
    "        apply_env_params,\n",
    "        in_axes=(None, 0),\n",
    "    )(\n",
    "        algo_cls,\n",
    "        env_params,\n",
    "    )\n",
    "\n",
    "    def eval_callback(algo, ts, rng):\n",
    "        act = algo.make_act(ts)\n",
    "        return evaluate(act, rng, env, algo.env_params, 50, max_steps_in_episode)\n",
    "\n",
    "    def get_returns(algo, ts, rng):\n",
    "        eval_info = eval_callback(algo, ts, rng)\n",
    "        return eval_info.length, eval_info.return_\n",
    "\n",
    "    algo = algo.replace(\n",
    "        eval_callback=get_returns\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    vmap_train = jax.jit(jax.vmap(algo_cls.train, in_axes=(0, 0)))\n",
    "    ts, (_, returns) = vmap_train(algo, keys)\n",
    "    returns.block_until_ready()\n",
    "\n",
    "    # Get expert actions\n",
    "    def get_expert_actions(ts, num_steps):\n",
    "        act = algo.make_act(ts)\n",
    "        def step(step_i: int, state: EvalState):\n",
    "            rng, rng_act, rng_step = jax.random.split(state.rng, 3)\n",
    "            action = act(state.trajectory[\"obss\"][step_i], rng_act)\n",
    "\n",
    "            state = EvalState(\n",
    "                rng,\n",
    "                None,\n",
    "                None,\n",
    "                trajectory=dict(\n",
    "                    obss=state.trajectory[\"obss\"],\n",
    "                    actions=state.trajectory[\"actions\"].at[step_i].set(action),\n",
    "                ),\n",
    "            )\n",
    "            return state\n",
    "\n",
    "        state = EvalState(\n",
    "            key,\n",
    "            None,\n",
    "            None,\n",
    "            trajectory=dict(\n",
    "                obss=ts.store_buffer.data.obs,\n",
    "                actions=ts.store_buffer.data.action,\n",
    "            )\n",
    "        )\n",
    "        state = jax.lax.fori_loop(\n",
    "            0,\n",
    "            num_steps,\n",
    "            step,\n",
    "            state,\n",
    "        )\n",
    "        return state.trajectory[\"actions\"]\n",
    "\n",
    "    expert_actions = jax.vmap(\n",
    "        jax.jit(get_expert_actions, static_argnames=[\"num_steps\"]),\n",
    "        in_axes=[0, None],\n",
    "    )(\n",
    "        ts,\n",
    "        ts.store_buffer.data.action.shape[1],\n",
    "    )\n",
    "\n",
    "    # \"Expert data\" from last PPO iteration\n",
    "    expert_trajectories = jax.vmap(eval_callback)(algo, ts, keys)\n",
    "\n",
    "    # Save data\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"buffer_info\": {k: v for k, v in ts.store_buffer.__dict__.items() if k != \"data\"},\n",
    "            \"learning_histories\": {\n",
    "                \"expert_action\": np.array(expert_actions),\n",
    "                **{k: np.array(v) for k, v in ts.store_buffer.data._asdict().items()}\n",
    "            },\n",
    "            \"algorithm\": {\n",
    "                \"algo\": algo_name,\n",
    "                **{k: v for k, v in config.items() if k != \"env\"},\n",
    "            },\n",
    "            \"env\": type(config[\"env\"]).__name__,\n",
    "            \"env_params\": {k: np.array(v) for k, v in env_params.items()},\n",
    "            \"observation_space\": env.observation_space(EnvParams()),\n",
    "            \"action_space\": env.action_space(EnvParams()),\n",
    "            \"expert_data\": {\n",
    "                k: np.array(v) for k, v in expert_trajectories.trajectory.items()\n",
    "            },\n",
    "        },\n",
    "        open(\"{}/learning_hist-{}-horizon={}-num_tasks={}-seed={}-{}.pkl\".format(\n",
    "            data_dir,\n",
    "            env_name,\n",
    "            max_steps_in_episode,\n",
    "            num_seeds,\n",
    "            seed_id,\n",
    "            algo_name,\n",
    "        ), \"wb\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331037a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets = (np.arange(ts.store_buffer.data.reward.shape[1])[None] + 1) * np.max(env_params, axis=-1, keepdims=True) - np.cumsum(ts.store_buffer.data.reward, axis=-1)\n",
    "\n",
    "for regret in regrets[:5]:\n",
    "    xrange = np.arange(len(regret))\n",
    "    plt.plot(xrange, regret)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd167f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_returns, env_param in zip(returns[:2], env_params):\n",
    "    xrange = np.arange(len(env_returns)) * config[\"eval_freq\"]\n",
    "    regret = np.max(env_param, axis=-1) - env_returns\n",
    "    print(regret.shape)\n",
    "    mean = np.mean(regret, axis=-1)\n",
    "    std = np.std(regret, axis=-1) / np.sqrt(regret.shape[-1])\n",
    "    plt.plot(xrange, mean)\n",
    "    plt.fill_between(xrange, mean - std, mean + std, alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.any(jnp.isnan(returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_returns in returns[:]:\n",
    "    xrange = np.arange(len(env_returns)) * config[\"eval_freq\"]\n",
    "    mean = np.mean(env_returns, axis=-1)\n",
    "    std = np.std(env_returns, axis=-1) / np.sqrt(env_returns.shape[-1])\n",
    "    plt.plot(xrange, mean)\n",
    "    plt.fill_between(xrange, mean - std, mean + std, alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbadc869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaai_rejax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
